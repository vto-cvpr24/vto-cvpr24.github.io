<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <title>CVPR 2024 Workshop: Virtual Try-On</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="data/default.css" rel="stylesheet" type="text/css" />
  <meta property='og:title' content='CVPR 2024 Workshop: Virtual Try-On' />
  <meta property='og:url' content='https://vto-cvpr24.github.io/' />
  <meta property='og:image' content='https://vto-cvpr24.github.io/data/teaser.jpg' />
  <meta property="og:type" content="website" />
</head>

<body>
  <div id="header">
    <div id="logo">
      <h1>
        <center>
          <span style="font-size:50%;color:#777;font-weight:normal">The first CVPR 2024 Workshop on</span><br>Virtual
          Try-On
        </center>
      </h1><br>
      <h2>
        <center>
          <span style="font-size:92%;color:#777;font-weight:normal">17th June, 2024, 1:30 PM to 6 PM @ CVPR 2024 in Seattle, CA,
            USA</span><br /><br />
          <span style="font-size:92%;color:#777;font-weight:normal">Venue: Seattle Convention Center, Room TBD</span>
        </center>
      </h2><br>
    </div>

    <div id="menu">
      <center>
        <ul>
          <li class="first"><a href="./index.html" accesskey="1">Home</a></li>
          <li><a href="./schedule.html" accesskey="2">Schedule</a></li>
        </ul>
      </center>
    </div>
    <div id="splash">
      <center><img src="data/seattle.jpg" alt="" width="880" /></center>
    </div>

    <div id="content">
      <h2>Overview</h2>
      <p>
        Virtual Try-On is an emerging consumer application that enables users to perceive products on their unique
        bodies in a virtual or mixed reality space.
        The retail e-commerce industry is beginning to heavily adopt these technologies within their offerings enabling
        their users to visualize products especially in the beauty, fashion, and accessories space before they make
        purchases, and provide opportunities to customize and personalize products.
        In principle, try-on experiences can have significant environmental impact by reducing the need to return
        products, improving satisfaction of purchased ones and improving accessibility.
        Enabling these applications requires solving diverse challenges in the space of computer vision, 3D modeling and
        reconstruction, geometry processing and learning, generative AI and perception.
        This is an active and multi-disciplinary area of research.
        <!--Further, the consumer-facing nature of this domain and inherent application to fashion requires researchers and
        technologists to understand the nature of bias in their data (catalog data is skewed towards a narrow bmi-range
        and demographic); trust and data-security issues (body image / data is often a necessary input) and the
        mental-health impact of try-on products for their users.-->

        The primary goal of this inaugral workshop is to bring together expert academic and industry researchers as well
        as
        young
        researchers working in this space to present, discuss and understand the state of the art and open challenges in
        this area that are core to enabling a convincing, useful and safe try-on experience.
      </p>
      <h2>Keynote Speakers</h2>
      <center>

        <table style="width:100%">
          <p>

            <tr>
              <td>
                <center><a href="https://www.cs.umd.edu/~lin/"> <img alt src="data/ming-lin.jpg" height="170" /> </a>
                </center>
              </td>
              <td>
                <center><a href="https://www.irakemelmacher.com/home"><img alt src="data/ira-kemelmacher-shlizerman.jpg"
                      height="170" /> </a></center>
              </td>
              <td>
                <center><a href="https://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html"><img alt
                      src="data/gerard-pons-moll.png" height="170" /> </a></center>
              </td>

              <td>
                <center><a href="https://sunilhadap.github.io/"> <img alt src="data/sunil-hadap.jpg" height="170" />
                  </a></center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <h3> Ming Lin</h3>
                </center>
              </td>
              <td>
                <center>
                  <h3> Ira Kemelmacher-Schlizerman</h3>
                </center>
              </td>
              <td>
                <center>
                  <h3> Gerard Pons-Moll</h3>
                </center>
              </td>
              <td>
                <center>
                  <h3> Sunil Hadap </h3>
                </center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <font size="2">UMD & Amazon</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">UW & Google</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">University of Tübingen</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">Amazon</font>
                </center>
              </td>
            </tr>
            



          </p>
        </table>
      </center>
      <br><br><br>

      <h2>Invited Short Talks</h2>
      <p></p>
      <center>
        <table  style="width:100%">
          <tr class="row_type_expandable">
            <td>
              <center> <strong> Title </strong> </center>
            </td>
            <td>
              <center> <strong> Presenter </strong> </center>
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>  M&M VTO: Multi-Garment Virtual Try-On and Editing </center>
            </td>
            <td>
              <center>  Luyang Zhu </br> <it> University of Washington </it> </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> We present M&M VTO–a mix and match virtual try-on method that takes as input multiple garment
                images, text description for garment layout and an image of a person. An example input includes: an image of a shirt, an
                image of a pair of pants, "rolled sleeves, shirt tucked in", and an image of a person. The output is a visualization of
                how those garments (in the desired layout) would look like on the given person. Key contributions of our method are: 1)
                a single stage diffusion based model, with no super resolution cascading, that allows to mix and match multiple garments
                at 1024x512 resolution preserving and warping intricate garment details, 2) architecture design (VTO UNet Diffusion
                Transformer) to disentangle denoising from person specific features, allowing for a highly effective finetuning strategy
                for identity preservation (6MB model per individual vs 4GB achieved with, e.g., dreambooth finetuning); solving a common
                identity loss problem in current virtual try-on methods, 3) layout control for multiple garments via text inputs
                specifically finetuned over PaLI-3 for virtual try-on task. Experimental results indicate that M&M VTO achieves
                state-of-the-art performance both qualitatively and quantitatively, as well as opens up new opportunities for virtual
                try-on via language-guided and multi-garment try-on.
              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>  
                Photorealistic virtual try-on from unconstrained designs 
              </center>
            </td>
            <td>
              <center>  Shuliang Ning & Xiaoguang Han </br> <it> The Chinese University of Hong Kong </it></center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> In this talk, we'll introduce a novel approach, ucVTON, for photorealistic virtual try-on of personalized clothing on
                human images. Unlike previous methods limited by input types, ours allows flexible style (text or image) and texture
                (full garment, cropped sections, or patches) specifications. To tackle the challenge of full garment entanglement, we
                use a two-stage pipeline to separate style and texture. We first generate a human parsing map for desired style and then
                composite textures onto it based on input. Our method introduces hierarchical CLIP features and position encoding in
                VTON for complex, non-stationary textures, setting a new standard in fashion editing.
              
            </td>
          </tr>

          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>  Mobile Fitting Room: On-device Virtual Try-on via Diffusion Models </center>
            </td>
            <td>
              <center>  Pratham Mehta </br> <it> Georgia Institute of Technology </it></center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> The growing digital landscape of fashion e-commerce calls for interactive and user-friendly
                interfaces for virtually trying on clothes. Traditional try-on methods grapple with challenges in adapting to diverse
                backgrounds, poses, and subjects. While newer methods, utilizing the recent advances of diffusion models, have achieved
                higher-quality image generation, the human-centered dimensions of mobile interface delivery and privacy concerns remain
                largely unexplored. We present Mobile Fitting Room, the first on-device diffusion-based virtual try-on system. To
                address multiple inter-related technical challenges such as high-quality garment placement and model compression for
                mobile devices, we present a novel technical pipeline and an interface design that enables privacy preservation and user
                customization. A usage scenario highlights how our tool can provide a seamless, interactive virtual try-on experience
                for customers and provide a valuable service for fashion e-commerce businesses.
              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>  Real-time Video Virtual Try-on Frameworks on mobile devices and data challenges </center>
            </td>
            <td>
              <center>  Ruowei (Irene) Jiang </br> <it> ModiFace </it> </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> With recent advances in content generation and rendering tasks using generative models, real-time video virtual try-on
                remains challenging, especially on mobile devices and web browsers. We present our framework and a series of works that
                bridge the gap between state-of-the-art neural networks and real-world challenges, constrained by device and data
                limitations.


              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     Makeup Prior Models for 3D Facial Makeup Estimation and Applications </center>
            </td>
            <td>
              <center>     Xingchao Yang </br> <it> CyberAgent & University of Tsukuba </it> </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> We introduce two types of makeup prior models—PCA-based and StyleGAN2-based—to enhance
                existing 3D face prior models. These priors are pivotal in estimating 3D makeup patterns from single makeup face images.
                Such patterns play a significant role in a broad spectrum of makeup-related applications, substantially enriching
                virtual try-on technologies with more realistic and customizable experiences. Our contributions support crucial
                functionalities, including 3D makeup face reconstruction, user-friendly makeup editing, makeup removal, makeup transfer,
                and interpolation.


              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     What limits the performance of makeup transfer? </center>
            </td>
            <td>
              <center>     Dr. Zhaoyang Sun  </br> <it> Wuhan University of Technology      </it>        </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> Makeup transfer aims to realistically and naturally reproduce diverse makeup styles onto a
                given face image. Due to the inherent unsupervised nature of makeup transfer, most previous approaches adopt the
                pseudo-ground-truth-guided strategy for model training. In this talk, we first reveal that the quality of the pseudo
                ground truth is the key factor limiting the performance of makeup transfer. Next, we propose a Content-Style Decoupled
                Makeup Transfer (CSD-MT) method, which works in a purely unsupervised manner and thus eliminates the negative effects of
                generating PGTs. Finally, extensive quantitative and qualitative analyses show the effectiveness of our CSD-MT method.


              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     Generating Animatable Layered Assets from a Single Scan </center>
            </td>
            <td>
              <center>     Taeksoo Kim & Byungjun Kim </br> <it> Seoul National University </it> </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> We present a framework that takes as input a single-layer clothed 3D human mesh and decomposes
                it into complete multi-layered 3D assets. The outputs can then be combined with other assets to create novel clothed
                human avatars with any pose. We first separate the input mesh using the 3D surface segmentation extracted from
                multi-view 2D segmentations. Then we synthesize the missing geometry of different layers in both posed and canonical
                spaces using a novel pose-guided Score Distillation Sampling (SDS) loss. Once we complete inpainting high-fidelity 3D
                geometry, we also apply the same SDS loss to its texture to obtain the complete appearance including the initially
                occluded regions. Through a series of decomposition steps, we obtain multiple layers of 3D assets in a shared canonical
                space normalized in terms of poses and human shapes, hence supporting effortless composition to novel identities and
                reanimation with novel poses.

              
            </td>
          </tr>
          <tr></tr>

          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On </center>
            </td>
            <td>
              <center>    Jeongho Kim </br> <it> Korea Advanced Institute of Science & Technology </it> </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> Given a clothing image and a person image, an image-based virtual try-on aims to generate a
                customized image that appears natural and accurately reflects the characteristics of the clothing image. In this
                presentation, we introduce StableVITON, learning the semantic correspondence between the clothing and the human body
                within the latent space of the pre-trained diffusion model in an end-to-end manner. Our proposed zero cross-attention
                blocks not only preserve the clothing details by learning the semantic correspondence but also generate high-fidelity
                images by utilizing the inherent knowledge of the pre-trained model in the warping process. Through our proposed novel
                attention total variation loss and applying augmentation, we achieve the sharp attention map, resulting in a more
                precise representation of clothing details. StableVITON shows state-of-the-art performance over existing virtual try-on
                models in both qualitative and quantitative results. Moreover, through the evaluation of a trained model on multiple
                datasets, StableVITON demonstrates its promising quality in a real-world setting.
              
            </td>
          </tr>
          <tr></tr>
          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     Integrating Learning-based Virtual Try-On in Fashion: Challenges and Advances
              </center>
            </td>
            <td>
              <center>
                    Lena Hong & Chaerin Kong </br> <it> NXN Labs </it>

              </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> Learning-based Virtual Try-On (VTO) has garnered significant attention for their potential to revolutionize the fashion
                industry. This field expands in impact across different segments of the fashion value chain, highlighting the distinct
                technical priorities from garment detail preservation for brands to size accuracy for consumers, and generative
                controllability in early design stages.
                As an emerging startup in this space, we address two principal challenges: visual hallucination and high-resolution
                synthesis. In effort to overcome the limitations of end-to-end feed-forward approach, we present our modular pipeline
                for visual hallucination, through a sequence of garment sanitization, signature component localization, and precise
                stitching. This methodology enhances the production quality by maintaining the fidelity of the garment's visual details.
                For high-resolution synthesis, we identify the inadequacy of conventional upsampling techniques in meeting the fashion
                industry's specific demands. To this end, we train our own super-resolution model, leveraging adversarial training to
                significantly improve texture detail and visual quality in upscaled images. We’re excited to share the ongoing
                challenges and our competitive efforts within the industry to push the boundaries of current methodologies.
              
            </td>
          </tr>
          <tr></tr>
          <tr class="row_type_expandable" onClick='toggleRow(this)'>
            <td>
              <center>     Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All
              </center>
            </td>
            <td>
              <center>
                Mehmet Saygin Seyfioglu </br> <it> University of Washington </it>

              </center>
            </td>
            <td class='expanded-row-content'>
              
                <strong>Abstract</strong> As online shopping is growing, the ability for buyers to virtually visualize products in their
                settings—a phenomenon we define as "Virtual Try-All"—has become crucial. Recent diffusion models inherently contain a
                world model, rendering them suitable for this task within an inpainting context. However, traditional image-conditioned
                diffusion models often fail to capture the fine-grained details of products. In contrast, personalization-driven models
                such as DreamPaint are good at preserving the item's details but they are not optimized for real-time applications. We
                present "Diffuse to Choose," a novel diffusion-based image-conditioned inpainting model that efficiently balances fast
                inference with the retention of high-fidelity details in a given reference item while ensuring accurate semantic
                manipulations in the given scene content. Our approach is based on incorporating fine-grained features from the
                reference image directly into the latent feature maps of the main diffusion model, alongside with a perceptual loss to
                further preserve the reference item's details. We conduct extensive testing on both in-house and publicly available
                datasets, and show that Diffuse to Choose is superior to existing zero-shot diffusion inpainting methods as well as
                few-shot diffusion personalization algorithms like DreamPaint.
              
            </td>
          </tr>
          <tr>

          </tr>

        </table>

      </center>
      <br><br><br>



      <h2>Organizers</h2>
      <p>
        <center>
          <table id="organizer" style="width:80%">
            <tr>
              <td>
                <center><a href="http://vid8687.github.io"><img alt src="data/vidya-narayanan.jpg" /></a>
                </center>
              </td>
              <td>
                <center><a href="https://sunilhadap.github.io/"> <img alt src="data/sunil-hadap.jpg" /> </a>
                </center>
              </td>
              <td>
                <center><a href="https://ps.is.mpg.de/person/jromero"> <img alt src="data/javier-romero.jpg" /> </a>
                </center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <h3>Vidya Narayanan</h3>
                </center>
              </td>
              <td>
                <center>
                  <h3>Sunil Hadap</h3>
                </center>
              </td>
              <td>
                <center>
                  <h3>Javier Romero</h3>
                </center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <font size="2">Amazon</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">Amazon</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">FAIR</font>
                </center>
              </td>
            </tr>

            <tr>
              <td>
                <center><a href="https://katiemlewis.github.io/"> <img alt src="data/kathleen-lewis.jpg" /> </a>
                </center>
              </td>
              <td>
                <center><a href="https://jhugestar.github.io/"> <img alt src="data/hanbyul-joo.jpg" /> </a>
                </center>
              </td>
              <td>
                <center><a href="https://www.cs.ubc.ca/~sheffa/"> <img alt src="data/alla-sheffer.jpg" /> </a>
                </center>
              </td>
              <td>
                <center><a href="https://www.cs.sfu.ca/~haoz/"> <img alt src="data/richard-zhang.png" /> </a>
                </center>
              </td>
            </tr>
            <tr>
              <td>
                <center>
                  <h3>Katie Lewis</h3>
                </center>
              </td>

              <td>
                <center>
                  <h3>Hanbyul Joo</h3>
                </center>
              </td>

              <td>
                <center>
                  <h3>Alla Sheffer</h3>
                </center>
              </td>

              <td>
                <center>
                  <h3>Hao (Richard) Zhang</h3>
                </center>
              </td>
            </tr>

            <tr>
              <td>
                <center>
                  <font size="2">Runway</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">SNU</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">UBC/Amazon</font>
                </center>
              </td>
              <td>
                <center>
                  <font size="2">SFU/Amazon</font>
                </center>
              </td>
            </tr>
          </table>
        </center>
      </p>

      <script>
        const toggleRow = (element) => {
          element.getElementsByClassName('expanded-row-content')[0].classList.toggle('hide-row');
          console.log(event);
        }
      </script>

      <h2>Contact Info</h2>
      <p>E-mail: vtocvpr24 AT gmail.com</p>

      <br></br></br>
      <div style="clear: both;">&nbsp;</div>
    </div>

    <p>Website based on https://futurecv.github.io/. Header image by <a
        href="https://commons.wikimedia.org/wiki/File:Seattle_Center_as_night_falls.jpg">Jeffery
        Hayes</a>, <a href="https://creativecommons.org/licenses/by-sa/3.0">CC BY-SA 3.0</a>, via Wikimedia Commons</p>
</body>


</html>
